{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "MachineLearningMastery Tutorial-checkpoint.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ani287/TextGenLSTM/blob/master/MachineLearningMastery_Tutorial_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcUtnflQTfMP",
        "colab_type": "code",
        "colab": {},
        "outputId": "5cc22085-abab-4e5a-e72e-8435b722888e"
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2W73n8yUuT1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "167f9086-c346-4dbc-a315-49abb33cdb8c"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMgDAzAlUwyS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "c231548f-c965-4081-da5f-f325fe52ab30"
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/bf/c28971266ca854a64f4b26f07c4112ddd61f30b4d1f18108b954a746f8ea/tensorflow_gpu-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2MB 32kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.2.1)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.2.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.34.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.30.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.5)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.17.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (47.3.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.6.0.post3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (4.1.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (4.6)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.6.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-WhqxlCVHkY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "39b06feb-620b-4d0f-8cce-01a3535bf400"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5lBdcIeVHfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa0i4jmoTfMU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "849e5713-2785-4551-ce81-36d006397edb"
      },
      "source": [
        "import numpy\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZH7ogoVZbPA",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "5c565a60-3d4a-4c25-bae3-ce630176133a"
      },
      "source": [
        "import pandas as pd \n",
        "from google.colab import files\n",
        "file = files.upload() "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-fe9211f9-9b1a-4fa4-bf1a-8bf16dc0b593\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-fe9211f9-9b1a-4fa4-bf1a-8bf16dc0b593\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving grimms_fairytales.txt to grimms_fairytales.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7RXUjgSTfMW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "7a9a9871-71c2-4d81-f2f4-49f91f85c372"
      },
      "source": [
        "f = open(\"grimms_fairytales.txt\", encoding=\"utf8\")\n",
        "document = f.read().lower()\n",
        "f.close()\n",
        "print('Corpus length:', len(document)) #545009\n",
        "print(document[:1000])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus length: 544009\n",
            ",title,text\t\t\t\n",
            "\"0,the golden bird,\"\"a certain king had a beautiful garden, and in the garden stood a tree\"\t\t\t\n",
            "which bore golden apples. these apples were always counted, and about\t\t\t\n",
            "the time when they began to grow ripe it was found that every night one\t\t\t\n",
            "of them was gone. the king became very angry at this, and ordered the\t\t\t\n",
            "gardener to keep watch all night under the tree. the gardener set his\t\t\t\n",
            "eldest son to watch\t but about twelve o’clock he fell asleep, and in\t\t\n",
            "the morning another of the apples was missing. then the second son was\t\t\t\n",
            "ordered to watch\t and at midnight he too fell asleep, and in the morning\t\t\n",
            "another apple was gone. then the third son offered to keep watch\t but\t\t\n",
            "the gardener at first would not let him, for fear some harm should come\t\t\t\n",
            "to him: however, at last he consented, and the young man laid himself\t\t\t\n",
            "under the tree to watch. as the clock struck twelve he heard a rustling\t\t\t\n",
            "noise in the air, and a bird came flying that was of pure gold\t and as\t\t\n",
            "it was s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWttxc5TTfMY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "070d3876-2a6d-487c-dd5a-597e1c978e27"
      },
      "source": [
        "document_new = document.split()\n",
        "fairytales = \" \".join(document_new)\n",
        "\n",
        "x = fairytales.find(\"*****\")\n",
        "print(x)\n",
        "\n",
        "text = fairytales[12:515323] \n",
        "print(text[:1000])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "515323\n",
            "\"0,the golden bird,\"\"a certain king had a beautiful garden, and in the garden stood a tree\" which bore golden apples. these apples were always counted, and about the time when they began to grow ripe it was found that every night one of them was gone. the king became very angry at this, and ordered the gardener to keep watch all night under the tree. the gardener set his eldest son to watch but about twelve o’clock he fell asleep, and in the morning another of the apples was missing. then the second son was ordered to watch and at midnight he too fell asleep, and in the morning another apple was gone. then the third son offered to keep watch but the gardener at first would not let him, for fear some harm should come to him: however, at last he consented, and the young man laid himself under the tree to watch. as the clock struck twelve he heard a rustling noise in the air, and a bird came flying that was of pure gold and as it was snapping at one of the apples with its beak, the garden\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "ok2Ki_WBTfMa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        },
        "outputId": "46b2d1a5-462f-4ebd-be44-09e0837fe988"
      },
      "source": [
        "#text = raw_text in tut\n",
        "\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "char_to_int"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 0,\n",
              " '!': 1,\n",
              " '\"': 2,\n",
              " '(': 3,\n",
              " ')': 4,\n",
              " '*': 5,\n",
              " ',': 6,\n",
              " '-': 7,\n",
              " '.': 8,\n",
              " '0': 9,\n",
              " '1': 10,\n",
              " '2': 11,\n",
              " '3': 12,\n",
              " '4': 13,\n",
              " '5': 14,\n",
              " '6': 15,\n",
              " '7': 16,\n",
              " '8': 17,\n",
              " '9': 18,\n",
              " ':': 19,\n",
              " '?': 20,\n",
              " '[': 21,\n",
              " ']': 22,\n",
              " '_': 23,\n",
              " 'a': 24,\n",
              " 'b': 25,\n",
              " 'c': 26,\n",
              " 'd': 27,\n",
              " 'e': 28,\n",
              " 'f': 29,\n",
              " 'g': 30,\n",
              " 'h': 31,\n",
              " 'i': 32,\n",
              " 'j': 33,\n",
              " 'k': 34,\n",
              " 'l': 35,\n",
              " 'm': 36,\n",
              " 'n': 37,\n",
              " 'o': 38,\n",
              " 'p': 39,\n",
              " 'q': 40,\n",
              " 'r': 41,\n",
              " 's': 42,\n",
              " 't': 43,\n",
              " 'u': 44,\n",
              " 'v': 45,\n",
              " 'w': 46,\n",
              " 'x': 47,\n",
              " 'y': 48,\n",
              " 'z': 49,\n",
              " '‘': 50,\n",
              " '’': 51,\n",
              " '“': 52,\n",
              " '”': 53}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-MsMhktTfMd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e9b3282d-0c51-456c-ca96-394d082d3aeb"
      },
      "source": [
        "# Now that the book has been loaded and the mapping prepared, we can summarize the dataset\n",
        "\n",
        "n_chars = len(text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars) \n",
        "print(\"Total Vocab: \", n_vocab) #54 distinct characters"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters:  515311\n",
            "Total Vocab:  54\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YCuSj5QTfMf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d581c5dd-f8e6-43c0-8dcc-21f390e63188"
      },
      "source": [
        "# need to define the training data for the network\n",
        "# a lot of flexibility in how you choose to break up the text and expose it to the network during training\n",
        "# here: the text is split up into subsequences with a fixed length of 100 characters, an arbitrary length\n",
        "\n",
        "# Each training pattern of the network is comprised of 100 time steps of one character (X) followed by one \n",
        "# character output (y)\n",
        "# When creating these sequences, we slide this window along the whole book one character at a time, allowing \n",
        "# each character a chance to be learned from the 100 characters that preceded it (except the first 100 characters of course)\n",
        "\n",
        "# As we split up the book into these sequences, we convert the characters to integers using our lookup table we prepared earlier\n",
        "\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "\n",
        "seq_length = 100 \n",
        "dataX = []\n",
        "dataY = []\n",
        "\n",
        "for i in range (0, n_chars - seq_length, 1):\n",
        "    seq_in = text[i:i + seq_length]\n",
        "    seq_out = text[i + seq_length]\n",
        "    dataX.append([char_to_int[char] for char in seq_in])\n",
        "    dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "\n",
        "# Now that we have prepared our training data we need to transform it so that it is suitable for use with Keras"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  515211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbxRJAr5TfMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to do\n",
        "\n",
        "# 1. transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network\n",
        "# 2. rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network --> sigmoid function by default\n",
        "# 3.  convert the output patterns (single characters converted to integers) into a one hot encoding\n",
        "\n",
        "# why? so that we can configure the network to predict the probability of each of the 54 different characters in the \n",
        "# vocabulary (an easier representation) rather than trying to force it to predict precisely the next character\n",
        "\n",
        "# Each y value is converted into a sparse vector with a length of 54, full of zeros except with a 1 in the column \n",
        "# for the letter (integer) that the pattern represents."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRbS3Di5TfMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "\n",
        "# one hot encode the output variable\n",
        "y = np_utils.to_categorical(dataY)\n",
        "\n",
        "# now can define our LSTM model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LajkO-2MTfMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a single hidden LSTM layer with 256 memory units\n",
        "# The network uses dropout with a probability of 20\n",
        "# The output layer is a Dense layer using the softmax activation function to output a probability prediction \n",
        "# for each of the 54 characters between 0 and 1\n",
        "\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faFDqXXiTfMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# There is no test dataset. We are modeling the entire training dataset to learn the probability of each character in a sequence\n",
        "\n",
        "# NOT interested in the most accurate (classification accuracy) model of the training dataset\n",
        "# This would be a model that predicts each character in the training dataset PERFECTLY\n",
        "# instead: interested in a generalization of the dataset that minimizes the chosen loss function\n",
        "# seeking a balance between generalization and overfitting but short of memorization\n",
        "\n",
        "# Because of the slowness and because of our optimization requirements --> use model checkpointing \n",
        "# to record all of the network weights to file each time an improvement in loss is observed at the end of the epoch\n",
        "\n",
        "\n",
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# now fit our model to the data. Here we use a modest number of 20 epochs and a large batch size of 128 patterns"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXvLp60zTfMp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "559d36e6-c84c-4c5f-c901-b95b73f99897"
      },
      "source": [
        "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "515211/515211 [==============================] - 956s 2ms/step - loss: 2.6612\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.66124, saving model to weights-improvement-01-2.6612.hdf5\n",
            "Epoch 2/20\n",
            "515211/515211 [==============================] - 955s 2ms/step - loss: 2.4493\n",
            "\n",
            "Epoch 00002: loss improved from 2.66124 to 2.44929, saving model to weights-improvement-02-2.4493.hdf5\n",
            "Epoch 3/20\n",
            "515211/515211 [==============================] - 979s 2ms/step - loss: 2.3187\n",
            "\n",
            "Epoch 00003: loss improved from 2.44929 to 2.31874, saving model to weights-improvement-03-2.3187.hdf5\n",
            "Epoch 4/20\n",
            "515211/515211 [==============================] - 985s 2ms/step - loss: 2.2161\n",
            "\n",
            "Epoch 00004: loss improved from 2.31874 to 2.21610, saving model to weights-improvement-04-2.2161.hdf5\n",
            "Epoch 5/20\n",
            "515211/515211 [==============================] - 989s 2ms/step - loss: 2.1359\n",
            "\n",
            "Epoch 00005: loss improved from 2.21610 to 2.13585, saving model to weights-improvement-05-2.1359.hdf5\n",
            "Epoch 6/20\n",
            "515211/515211 [==============================] - 981s 2ms/step - loss: 2.0711\n",
            "\n",
            "Epoch 00006: loss improved from 2.13585 to 2.07115, saving model to weights-improvement-06-2.0711.hdf5\n",
            "Epoch 7/20\n",
            "515211/515211 [==============================] - 991s 2ms/step - loss: 2.0181\n",
            "\n",
            "Epoch 00007: loss improved from 2.07115 to 2.01810, saving model to weights-improvement-07-2.0181.hdf5\n",
            "Epoch 8/20\n",
            "515211/515211 [==============================] - 994s 2ms/step - loss: 1.9764\n",
            "\n",
            "Epoch 00008: loss improved from 2.01810 to 1.97640, saving model to weights-improvement-08-1.9764.hdf5\n",
            "Epoch 9/20\n",
            "515211/515211 [==============================] - 958s 2ms/step - loss: 1.9409\n",
            "\n",
            "Epoch 00009: loss improved from 1.97640 to 1.94089, saving model to weights-improvement-09-1.9409.hdf5\n",
            "Epoch 10/20\n",
            "515211/515211 [==============================] - 951s 2ms/step - loss: 1.9085\n",
            "\n",
            "Epoch 00010: loss improved from 1.94089 to 1.90854, saving model to weights-improvement-10-1.9085.hdf5\n",
            "Epoch 11/20\n",
            "515211/515211 [==============================] - 960s 2ms/step - loss: 1.8812\n",
            "\n",
            "Epoch 00011: loss improved from 1.90854 to 1.88122, saving model to weights-improvement-11-1.8812.hdf5\n",
            "Epoch 12/20\n",
            "515211/515211 [==============================] - 971s 2ms/step - loss: 1.8582\n",
            "\n",
            "Epoch 00012: loss improved from 1.88122 to 1.85816, saving model to weights-improvement-12-1.8582.hdf5\n",
            "Epoch 13/20\n",
            "515211/515211 [==============================] - 978s 2ms/step - loss: 1.8370\n",
            "\n",
            "Epoch 00013: loss improved from 1.85816 to 1.83704, saving model to weights-improvement-13-1.8370.hdf5\n",
            "Epoch 14/20\n",
            "515211/515211 [==============================] - 971s 2ms/step - loss: 1.8170\n",
            "\n",
            "Epoch 00014: loss improved from 1.83704 to 1.81698, saving model to weights-improvement-14-1.8170.hdf5\n",
            "Epoch 15/20\n",
            "515211/515211 [==============================] - 952s 2ms/step - loss: 1.7994\n",
            "\n",
            "Epoch 00015: loss improved from 1.81698 to 1.79945, saving model to weights-improvement-15-1.7994.hdf5\n",
            "Epoch 16/20\n",
            "515211/515211 [==============================] - 944s 2ms/step - loss: 1.7864\n",
            "\n",
            "Epoch 00016: loss improved from 1.79945 to 1.78640, saving model to weights-improvement-16-1.7864.hdf5\n",
            "Epoch 17/20\n",
            "515211/515211 [==============================] - 932s 2ms/step - loss: 1.7717\n",
            "\n",
            "Epoch 00017: loss improved from 1.78640 to 1.77171, saving model to weights-improvement-17-1.7717.hdf5\n",
            "Epoch 18/20\n",
            "515211/515211 [==============================] - 929s 2ms/step - loss: 1.7584\n",
            "\n",
            "Epoch 00018: loss improved from 1.77171 to 1.75844, saving model to weights-improvement-18-1.7584.hdf5\n",
            "Epoch 19/20\n",
            "515211/515211 [==============================] - 889s 2ms/step - loss: 1.7457\n",
            "\n",
            "Epoch 00019: loss improved from 1.75844 to 1.74574, saving model to weights-improvement-19-1.7457.hdf5\n",
            "Epoch 20/20\n",
            "515211/515211 [==============================] - 889s 2ms/step - loss: 1.7362\n",
            "\n",
            "Epoch 00020: loss improved from 1.74574 to 1.73621, saving model to weights-improvement-20-1.7362.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f15c012cbe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO0ji-k2TfMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generating text using the trained LSTM network is relatively straightforward\n",
        "\n",
        "# load the network weights\n",
        "filename = \"weights-improvement-19-1.9435.hdf5\" # look where loss is the smallest!\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wV3q6mJTfMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preparing the mapping of unique characters to integers, we must also create a reverse mapping that we can \n",
        "# use to convert the integers back to characters so that we can understand the predictions\n",
        "\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCRq_8yxTfMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# first start off with a seed sequence as input, generate the next character then update the seed \n",
        "# sequence to add the generated character on the end and trim off the first character\n",
        "# This process is repeated for as long as we want to predict new characters (e.g. a sequence of 1,000 characters in length)\n",
        "\n",
        "\n",
        "# pick a random seed\n",
        "start = numpy.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "    x = x / float(n_vocab)\n",
        "    prediction = model.predict(x, verbose=0)\n",
        "    index = numpy.argmax(prediction)\n",
        "    result = int_to_char[index]\n",
        "    seq_in = [int_to_char[value] for value in pattern]\n",
        "    sys.stdout.write(result)\n",
        "    pattern.append(index)\n",
        "    pattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezJsRXBATfMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# larger model xxx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQE80B6iTfMy",
        "colab_type": "text"
      },
      "source": [
        "### 10 Extension Ideas to Improve the Model\n",
        "Below are 10 ideas that may further improve the model that you could experiment with are:\n",
        "\n",
        "* Predict fewer than 1,000 characters as output for a given seed.\n",
        "* Remove all punctuation from the source text, and therefore from the models’ vocabulary.\n",
        "* Try a one hot encoded for the input sequences.\n",
        "* Train the model on padded sentences rather than random sequences of characters.\n",
        "* Increase the number of training epochs to 100 or many hundreds.\n",
        "* Add dropout to the visible input layer and consider tuning the dropout percentage.\n",
        "* Tune the batch size, try a batch size of 1 as a (very slow) baseline and larger sizes from there.\n",
        "* Add more memory units to the layers and/or more layers.\n",
        "* Experiment with scale factors (temperature) when interpreting the prediction probabilities.\n",
        "* Change the LSTM layers to be “stateful” to maintain state across batches."
      ]
    }
  ]
}